---
#
# Applies configuration to a cluster post installation
#
# Performs:
#  * Creates infra node machineset
#  * Creates workload node machineset
#  * Moves infra node pods/workload to infra nodes
#

- name: Get cluster name
  shell: |
    {%raw%}oc get machineset -n openshift-machine-api -o=go-template='{{(index (index .items 0).metadata.labels {%endraw%} "{{ machineset_metadata_label_prefix }}/cluster-api-cluster" {%raw%})}}'{%endraw%}
  register: cluster_name
  environment:
    KUBECONFIG: "{{ kubeconfig_path }}"

- name: Get cluster version
  command: |
    {%raw%}oc get clusterversion version -o go-template --template="{{.status.desired.version}}"{%endraw%}
  register: ocp_version
  environment:
    KUBECONFIG: "{{ kubeconfig_path }}"

- name: Set userDataSecret name
  set_fact:
    user_data_secret: "worker-user-data"

- name: Index install log
  script:
    cmd: index-install-data.sh
  environment:
    ES_SERVER: "{{ es_server }}"
    INSTALL_LOG: "{{ ansible_user_dir }}/{{ dynamic_deploy_path }}/.openshift_install.log"

- name: AWS Block of tasks
  block:
    - name: (AWS) Get AMI ID
      shell: |
        {%raw%}oc get machineset -n openshift-machine-api -o=go-template='{{(index .items 0).spec.template.spec.providerSpec.value.ami.id}}'{%endraw%}
      register: ami_id
      environment:
        KUBECONFIG: "{{ kubeconfig_path }}"

    - name: (AWS) Get cluster region
      shell: |
        {%raw%}oc get machineset -n openshift-machine-api -o=go-template='{{(index .items 0).spec.template.spec.providerSpec.value.placement.region}}'{%endraw%}
      register: aws_region
      environment:
        KUBECONFIG: "{{ kubeconfig_path }}"

    - name: (AWS) Template out machineset yamls
      template:
        src: "{{item.src}}"
        dest: "{{item.dest}}"
      when: item.toggle|bool
      with_items:
        - src: aws-infra-node-machineset.yml.j2
          dest: "{{ ansible_user_dir }}/{{ dynamic_deploy_path }}/infra-node-machineset.yml"
          toggle: "{{openshift_toggle_infra_node}}"
        - src: aws-workload-node-machineset.yml.j2
          dest: "{{ ansible_user_dir }}/{{ dynamic_deploy_path }}/workload-node-machineset.yml"
          toggle: "{{openshift_toggle_workload_node}}"
  when: platform == "aws"

- name: Azure Block of tasks
  block:
    - name: (Azure) Get AMI ID
      shell: |
        {%raw%}oc get machineset -n openshift-machine-api -o=go-template='{{(index .items 0).spec.template.spec.providerSpec.value.ami.id}}'{%endraw%}
      register: ami_id
      environment:
        KUBECONFIG: "{{ kubeconfig_path }}"

    - name: (Azure) Get cluster location (region)
      shell: |
        {%raw%}oc get machineset -n openshift-machine-api -o=go-template='{{(index .items 0).spec.template.spec.providerSpec.value.location}}'{%endraw%}
      register: azure_location
      environment:
        KUBECONFIG: "{{ kubeconfig_path }}"

    - name: (Azure) Template out machineset yamls
      template:
        src: "{{item.src}}"
        dest: "{{item.dest}}"
      when: item.toggle|bool
      with_items:
        - src: azure-infra-node-machineset.yml.j2
          dest: "{{ ansible_user_dir }}/{{ dynamic_deploy_path }}/infra-node-machineset.yml"
          toggle: "{{openshift_toggle_infra_node}}"
        - src: azure-workload-node-machineset.yml.j2
          dest: "{{ ansible_user_dir }}/{{ dynamic_deploy_path }}/workload-node-machineset.yml"
          toggle: "{{openshift_toggle_workload_node}}"
  when: platform == "azure"

- name: GCP block of tasks
  block:
    - name: Get machineset name of a worker node
      shell: |
        {%raw%}oc get machinesets --no-headers -n openshift-machine-api | awk {'print $1'} | awk 'NR==1{print $1}'{%endraw%}
      register: worker_node_machineset
      environment:
        KUBECONFIG: "{{ kubeconfig_path }}"

    - name: Get machineset image from worker node
      shell: |
       oc get machineset {{ worker_node_machineset.stdout }} -n openshift-machine-api -o jsonpath='{.spec.template.spec.providerSpec.value.disks[0].image}'
      register: worker_machineset_image
      environment:
        KUBECONFIG: "{{ kubeconfig_path }}"

    - name: (GCP) Template out machineset yamls
      template:
        src: "{{item.src}}"
        dest: "{{item.dest}}"
      when: item.toggle|bool
      with_items:
        - src: gcp-infra-node-machineset.yml.j2
          dest: "{{ ansible_user_dir }}/{{ dynamic_deploy_path }}/infra-node-machineset.yml"
          toggle: "{{openshift_toggle_infra_node}}"
        - src: gcp-workload-node-machineset.yml.j2
          dest: "{{ ansible_user_dir }}/{{ dynamic_deploy_path }}/workload-node-machineset.yml"
          toggle: "{{openshift_toggle_workload_node}}"
        - src: gcp-ssd-sc.yml.j2
          dest: "{{ ansible_user_dir }}/{{ dynamic_deploy_path }}/pd-ssd-sc.yml"
          toggle: true

    - name: (GCP) Create faster ssd sc
      shell: |
        oc create -f {{ item.sc }}
      with_items:
        - sc: "{{ ansible_user_dir }}/{{ dynamic_deploy_path }}/pd-ssd-sc.yml"
      environment:
        KUBECONFIG: "{{ kubeconfig_path }}"

  when: platform == "gcp"

- name: (OSP) Template out machineset yamls
  template:
    src: "{{item.src}}"
    dest: "{{item.dest}}"
  with_items:
    - src: osp-infra-node-machineset.yml.j2
      dest: "{{ ansible_user_dir }}/{{ dynamic_deploy_path }}/infra-node-machineset.yml"
      toggle: "{{openshift_toggle_infra_node}}"
    - src: osp-workload-node-machineset.yml.j2
      dest: "{{ ansible_user_dir }}/{{ dynamic_deploy_path }}/workload-node-machineset.yml"
      toggle: "{{openshift_toggle_workload_node}}"
  when:
    - item.toggle|bool
    - platform == "osp"

- name: OVN block
  block:
  - name: Scale CVO Down for OVNKubernetes image patching
    command: oc scale -n openshift-cluster-version deployment.apps/cluster-version-operator --replicas=0
  
  - name: Patch new OVNKubernetes Image
    command: oc -n openshift-network-operator set env deployment.apps/network-operator OVN_IMAGE={{openshift_ovn_image}}
  
  - name: Pause the rollout to begin
    pause:
      seconds: 10
  
  - name: Wait for OVNK new node images to roll out
    command: oc rollout status -n {{ ovn_namespace }} ds/ovnkube-node
  
  - name: Wait for OVNK new master images to roll out
    command: oc rollout status -n {{ ovn_namespace }} ds/ovnkube-master
  environment:
    KUBECONFIG: "{{ kubeconfig_path }}"
  when: openshift_toggle_ovn_patch|bool

- name: OVN SBDB block
  block:
  - name: Deploy ovnkube-sbdb-relay
   script: sbdb-relay.sh deploy

  - name: Wait for OVNK sbdb-relay images to roll out
    command: oc rollout status -n {{ ovn_namespace }} deployment.apps/ovnkube-sbdb-relay

  - name: Verify ovnkube-sbdb-relay connectivity
    script: sbdb-relay.sh verify
  environment:
    KUBECONFIG: "{{ kubeconfig_path }}"
  when: openshift_toggle_ovn_relay|bool

- name: Create machinesets
  shell: |
    oc create -f {{item.ms}}
  with_items:
    - ms: "{{ ansible_user_dir }}/{{ dynamic_deploy_path }}/infra-node-machineset.yml"
      toggle: "{{openshift_toggle_infra_node}}"
    - ms: "{{ ansible_user_dir }}/{{ dynamic_deploy_path }}/workload-node-machineset.yml"
      toggle: "{{openshift_toggle_workload_node}}"
  environment:
    KUBECONFIG: "{{ kubeconfig_path }}"
  when: item.toggle|bool

- name: Set expected node count
  set_fact:
    expected_node_count: "{{openshift_worker_count|int + openshift_master_count|int}}"

- name: Increment expected node count with infra nodes
  set_fact:
    expected_node_count: "{{expected_node_count|int + 3}}"
  when: openshift_toggle_infra_node|bool

- name: Increment expected node count with workload node
  set_fact:
    expected_node_count: "{{expected_node_count|int + 1}}"
  when: openshift_toggle_workload_node|bool

- name: Poll nodes to see if creating nodes finished
  shell: oc get nodes | grep " Ready" -ic
  register: current_node_count
  until: current_node_count.stdout|int >= (expected_node_count|int)
  delay: 1
  retries: "{{openshift_post_install_poll_attempts|int}}"
  environment:
    KUBECONFIG: "{{ kubeconfig_path }}"
  when: (openshift_toggle_infra_node|bool == true or openshift_toggle_workload_node|bool == true)

- name: Relabel the infra nodes
  shell: |
    oc label nodes --overwrite -l 'node-role.kubernetes.io/infra=' node-role.kubernetes.io/worker-
  environment:
    KUBECONFIG: "{{ kubeconfig_path }}"
  when: openshift_toggle_infra_node|bool

- name: Relabel the workload node
  shell: |
    oc label nodes --overwrite -l 'node-role.kubernetes.io/workload=' node-role.kubernetes.io/worker-
  environment:
    KUBECONFIG: "{{ kubeconfig_path }}"
  when: openshift_toggle_workload_node|bool

- name: Add additional label to worker nodes to provide ablity to isolate workloads on workers
  shell: |
    oc label nodes --overwrite -l 'node-role.kubernetes.io/worker=' computenode=true
  environment:
    KUBECONFIG: "{{ kubeconfig_path }}"

# Due to upgrade testing we will no longer disable the CVO
# - name: Disable CVO to prevent squashed configuration changes to cluster operators
#   shell: |
#     oc scale --replicas 0 -n openshift-cluster-version deployments/cluster-version-operator

- name: Taint the workload node
  shell: |
    oc adm taint node -l node-role.kubernetes.io/workload= role=workload:NoSchedule --overwrite=true
  environment:
    KUBECONFIG: "{{ kubeconfig_path }}"
  when: openshift_toggle_workload_node|bool

- block:
  - name: Copy new cluster-monitoring-config
    template:
      src: cluster-monitoring-config.yml.j2
      dest: "{{ ansible_user_dir }}/{{ dynamic_deploy_path }}/cluster-monitoring-config.yml"
    register: copy_yaml
  
  - set_fact:
      cluster_monitoring_yaml_path: "{{ copy_yaml.dest }}"

  - name: Replace the cluster-monitoring-config ConfigMap
    shell: |
      oc create -f {{ cluster_monitoring_yaml_path }}
    ignore_errors: yes
    environment:
      KUBECONFIG: "{{ kubeconfig_path }}"
  when: openshift_toggle_infra_node|bool

# Need this to steer cluster-monitoring-operator onto the infra nodes
#- block:
#   - name: Add override for cluster-monitoring-operator to disable CVO management
#     copy:
#       src: version-patch-override.yaml
#       dest: "/tmp/version-patch-override.yaml"

#   - name: Patch the clusteversion with the overrides
#     shell: |
#       oc patch clusterversion version --type json -p "$(cat /tmp/version-patch-override.yaml)"

#   - name: Remove the existing node selector from the cluster-monitoring-operator deployment config
#     shell: |
#       oc patch deployment.apps/cluster-monitoring-operator -n openshift-monitoring --type json -p '[{"op": "remove", "path": "/spec/template/spec/nodeSelector"}]'
#  when: openshift_toggle_infra_node|bool

# Attempting to migrate these operators seems to break upgrades
# - name: Remove existing nodeSelector from ingress-operator
#   shell: |
#     oc patch deployment.apps/ingress-operator -n openshift-ingress-operator --type json -p '[{"op": "remove", "path": "/spec/template/spec/nodeSelector"}]'

# - name: Remove existing nodeSelector from monitoring-operator
#   shell: |
#     oc patch deployment.apps/cluster-monitoring-operator -n openshift-monitoring --type json -p '[{"op": "remove", "path": "/spec/template/spec/nodeSelector"}]'

# - name: Remove existing nodeSelector from registry-operator
#   shell: |
#     oc patch deployment.apps/cluster-image-registry-operator -n openshift-image-registry --type json -p '[{"op": "remove", "path": "/spec/template/spec/nodeSelector"}]'

- name: Apply new nodeSelector to infra workload components
  shell: |
    oc patch {{item.object}} {{item.type|default('',True)}} -n {{item.namespace}} -p {{item.patch}}
  with_items:
    # Due to CVO remaining enabled for upgrades we can not migrate the cluster-ingress-operator
    # # Ingress/Router (Relocate from worker nodes) - Does require CVO Disable
    # - namespace: openshift-ingress-operator
    #   object: deployment.apps/ingress-operator
    #   patch: |
    #     '{"spec": {"template": {"spec": {"nodeSelector": {"node-role.kubernetes.io/infra": ""}}}}}'
    - namespace: openshift-ingress-operator
      object: ingresscontrollers/default
      patch: |
        '{"spec": {"nodePlacement": {"nodeSelector": {"matchLabels": {"node-role.kubernetes.io/infra": ""}}}}}'
      type: "--type merge"
    # Monitoring (Relocate cluster-monitoring-operator from worker nodes) - Requires CVO overriding
    #- namespace: openshift-monitoring
    #  object: deployment.apps/cluster-monitoring-operator
    #  patch: |
    #    '{"spec": {"template": {"spec": {"nodeSelector": {"node-role.kubernetes.io/infra": ""}}}}}'
      # Registry (Relocate from master nodes) - Does not require CVO Disable
      # - namespace: openshift-image-registry
      #   object: deployment.apps/cluster-image-registry-operator
      #   patch: |
      #     '{"spec": {"template": {"spec": {"nodeSelector": {"node-role.kubernetes.io/infra": ""}}}}}'

    - namespace: openshift-image-registry
      object: deployment.apps/image-registry
      patch: |
         '{"spec": {"template": {"spec": {"nodeSelector": {"node-role.kubernetes.io/infra": ""}}}}}'

     ## Logging (If it is installed)
     # - namespace: openshift-logging
     #   object: deployment.apps/cluster-logging-operator
     #   patch: |
     #     '{"spec": {"template": {"spec": {"nodeSelector": {"node-role.kubernetes.io/infra": ""}}}}}'
     # - namespace: openshift-logging
     #   object: deployment.apps/elasticsearch-clientdatamaster-0-1
     #   patch: |
     #     '{"spec": {"template": {"spec": {"nodeSelector": {"node-role.kubernetes.io/infra": ""}}}}}'
     # - namespace: openshift-logging
     #   object: deployment.apps/elasticsearch-operator
     #   patch: |
     #     '{"spec": {"template": {"spec": {"nodeSelector": {"node-role.kubernetes.io/infra": ""}}}}}'
     # - namespace: openshift-logging
     #   object: deployment.apps/kibana
     #   patch: |
     #     '{"spec": {"template": {"spec": {"nodeSelector": {"node-role.kubernetes.io/infra": ""}}}}}'
  environment:
    KUBECONFIG: "{{ kubeconfig_path }}"
  when: openshift_toggle_infra_node|bool

- name: Use sincgars to enable remote write
  block:
    - name: clone sincgars
      git:
        repo: 'https://github.com/aakarshg/sincgars.git'
        dest: "{{ ansible_user_dir }}/sincgars"
        force: yes

    - name: (sincgars_cluster_name not provided) run sincgars
      command: python deploy.py -c {{ cluster_name.stdout }} -u {{ sincgars_remote_write_url }}
      args:
        chdir: "{{ ansible_user_dir }}/sincgars"
      when: sincgars_cluster_name == ""

    - name: (sincgars_cluster_name provided) run sincgars
      command: python deploy.py -c {{ sincgars_cluster_name }} -u {{ sincgars_remote_write_url }}
      args:
        chdir: "{{ ansible_user_dir }}/sincgars"
      when: sincgars_cluster_name != ""
  when: sincgars_enable and sincgars_remote_write_url != ""

- name: Deploy dittybopper
  block:
    - name: clone dittybopper
      git:
        repo: 'https://github.com/cloud-bulldozer/performance-dashboards.git'
        dest: "{{ ansible_user_dir }}/performance-dashboards"
        force: yes

    - name: Deploy mutable Grafana
      command: ./deploy.sh
      args:
        chdir: "{{ ansible_user_dir }}/performance-dashboards/dittybopper"
      environment:
        KUBECONFIG: "{{ kubeconfig_path }}"
  when: dittybopper_enable


- name: Deploy promtail
  block:
    - name: Template Promtail Values
      template:
        src: promtail-values.yml.j2
        dest: "{{ ansible_user_dir }}/{{ dynamic_deploy_path }}/promtail.yaml"
    - name: Install Promtail
      shell: |
        helm repo add grafana https://grafana.github.io/helm-charts 
        helm repo update
        oc create namespace loki || true
        oc adm policy add-scc-to-group privileged system:authenticated
        helm upgrade --install promtail --namespace=loki grafana/promtail -f {{ ansible_user_dir }}/{{ dynamic_deploy_path }}/promtail.yaml
      environment:
        KUBECONFIG: "{{ kubeconfig_path }}"
  when: loki_enable

  
